{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pypdf docx2txt chromadb sentence-transformers openai pyngrok -q\n"
      ],
      "metadata": {
        "id": "6YEhMEsRYPRq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_code = \"\"\"\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import time\n",
        "import uuid\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# --- Optional dependencies, imported lazily where possible ---\n",
        "try:\n",
        "    from pypdf import PdfReader\n",
        "except Exception:\n",
        "    PdfReader = None\n",
        "\n",
        "try:\n",
        "    import docx2txt\n",
        "except Exception:\n",
        "    docx2txt = None\n",
        "\n",
        "try:\n",
        "    import chromadb\n",
        "    from chromadb.config import Settings\n",
        "except Exception:\n",
        "    chromadb = None\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    SentenceTransformer = None\n",
        "\n",
        "# OpenAI is optional; we also support a local fallback via Ollama if present\n",
        "OPENAI_IMPORT_ERROR = None\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception as e:\n",
        "    OPENAI_IMPORT_ERROR = e\n",
        "    OpenAI = None\n",
        "\n",
        "# Utilities\n",
        "\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    text: str\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "def _clean_text(txt: str) -> str:\n",
        "    txt = txt.replace(\"\\\\x00\", \" \").replace(\"\\\\u0000\", \" \")\n",
        "    txt = re.sub(r\"[ \\\\t]+\", \" \", txt)\n",
        "    txt = re.sub(r\"\\\\n{3,}\", \"\\\\n\\\\n\", txt)\n",
        "    return txt.strip()\n",
        "\n",
        "SECTION_PATTERNS = [\n",
        "    r\"(?m)^(Section|Sec\\\\.?)\\\\s+(\\\\d+(\\\\.\\\\d+)*)[:\\\\.\\\\-\\\\s]\",\n",
        "    r\"(?m)^(Clause)\\\\s+(\\\\d+(\\\\.\\\\d+)*)[:\\\\.\\\\-\\\\s]\",\n",
        "    r\"(?m)^(Article)\\\\s+(\\\\d+(\\\\.\\\\d+)*)[:\\\\.\\\\-\\\\s]\",\n",
        "    r\"(?m)^\\\\d+(\\\\.\\\\d+)*\\\\s+[A-Z][^\\\\n]{0,80}$\",\n",
        "    r\"(?m)^[A-Z][A-Z \\\\-]{3,}$\",\n",
        "]\n",
        "\n",
        "def detect_sections(text: str) -> List[Tuple[str, Tuple[int, int]]]:\n",
        "    spans = []\n",
        "    headings = []\n",
        "    for pat in SECTION_PATTERNS:\n",
        "        for m in re.finditer(pat, text):\n",
        "            start = m.start()\n",
        "            line_start = text.rfind(\"\\\\n\", 0, start) + 1\n",
        "            line_end = text.find(\"\\\\n\", start)\n",
        "            if line_end == -1:\n",
        "                line_end = len(text)\n",
        "            title = text[line_start:line_end].strip()\n",
        "            if title and (line_start, line_end, title) not in headings:\n",
        "                headings.append((line_start, line_end, title))\n",
        "    if not headings:\n",
        "        return [(\"Document\", (0, len(text)))]\n",
        "    headings = sorted(headings, key=lambda x: x[0])\n",
        "    for idx, (s, e, title) in enumerate(headings):\n",
        "        next_start = headings[idx + 1][0] if idx + 1 < len(headings) else len(text)\n",
        "        spans.append((title, (s, next_start)))\n",
        "    dedup = []\n",
        "    last_end = -1\n",
        "    for title, (s, e) in spans:\n",
        "        if s >= last_end:\n",
        "            dedup.append((title, (s, e)))\n",
        "            last_end = e\n",
        "    return dedup or [(\"Document\", (0, len(text)))]\n",
        "\n",
        "def split_into_chunks(text: str, max_chars: int = 1200, overlap: int = 150) -> List[str]:\n",
        "    paras = [p.strip() for p in text.split(\"\\\\n\\\\n\") if p.strip()]\n",
        "    chunks = []\n",
        "    cur = \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 2 <= max_chars:\n",
        "            cur = (cur + \"\\\\n\\\\n\" + p).strip()\n",
        "        else:\n",
        "            if cur:\n",
        "                chunks.append(cur)\n",
        "            if len(p) > max_chars:\n",
        "                start = 0\n",
        "                while start < len(p):\n",
        "                    end = min(start + max_chars, len(p))\n",
        "                    chunks.append(p[start:end])\n",
        "                    start = end - overlap if end - overlap > start else end\n",
        "                cur = \"\"\n",
        "            else:\n",
        "                cur = p\n",
        "    if cur:\n",
        "        chunks.append(cur)\n",
        "    with_overlap = []\n",
        "    for i, ch in enumerate(chunks):\n",
        "        if i == 0:\n",
        "            with_overlap.append(ch)\n",
        "        else:\n",
        "            prev = chunks[i-1]\n",
        "            tail = prev[-overlap:]\n",
        "            merged = (tail + \"\\\\n\\\\n\" + ch).strip()\n",
        "            with_overlap.append(merged)\n",
        "    return with_overlap\n",
        "\n",
        "def extract_text_from_pdf(file_bytes: bytes) -> Tuple[str, Dict[int, str]]:\n",
        "    if PdfReader is None:\n",
        "        raise RuntimeError(\"pypdf not installed\")\n",
        "    reader = PdfReader(io.BytesIO(file_bytes))\n",
        "    text_pages = {}\n",
        "    all_text = []\n",
        "    for i, page in enumerate(reader.pages):\n",
        "        try:\n",
        "            t = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            t = \"\"\n",
        "        t = _clean_text(t)\n",
        "        text_pages[i + 1] = t\n",
        "        all_text.append(t)\n",
        "    return _clean_text(\"\\\\n\\\\n\".join(all_text)), text_pages\n",
        "\n",
        "def extract_text_from_docx(file_bytes: bytes) -> str:\n",
        "    if docx2txt is None:\n",
        "        raise RuntimeError(\"docx2txt not installed\")\n",
        "    tmp = io.BytesIO(file_bytes)\n",
        "    import tempfile\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".docx\", delete=False) as fp:\n",
        "        fp.write(tmp.read())\n",
        "        temp_path = fp.name\n",
        "    try:\n",
        "        txt = docx2txt.process(temp_path) or \"\"\n",
        "    finally:\n",
        "        try:\n",
        "            os.unlink(temp_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return _clean_text(txt)\n",
        "\n",
        "def build_embeddings_model(name: str):\n",
        "    if SentenceTransformer is None:\n",
        "        raise RuntimeError(\"sentence-transformers not installed\")\n",
        "    return SentenceTransformer(name)\n",
        "\n",
        "def ensure_vector_store(persist_dir: str):\n",
        "    if chromadb is None:\n",
        "        raise RuntimeError(\"chromadb not installed\")\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "    client = chromadb.PersistentClient(path=persist_dir, settings=Settings(allow_reset=True))\n",
        "    return client\n",
        "\n",
        "def add_to_collection(client, collection_name: str, embeddings_model, chunks: List[Chunk]):\n",
        "    collection = client.get_or_create_collection(collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
        "    texts = [c.text for c in chunks]\n",
        "    metadatas = [c.metadata for c in chunks]\n",
        "    ids = [c.metadata.get(\"id\", str(uuid.uuid4())) for c in chunks]\n",
        "    batch = 64\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch):\n",
        "        embs = embeddings_model.encode(texts[i:i+batch], show_progress_bar=False, normalize_embeddings=True)\n",
        "        all_embeddings.extend(embs)\n",
        "    collection.add(documents=texts, embeddings=all_embeddings, metadatas=metadatas, ids=ids)\n",
        "    return collection\n",
        "\n",
        "def retrieve(client, collection_name: str, embeddings_model, query: str, top_k: int = 6) -> List[Chunk]:\n",
        "    collection = client.get_or_create_collection(collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
        "    q_emb = embeddings_model.encode([query], normalize_embeddings=True)[0]\n",
        "    res = collection.query(query_embeddings=[q_emb], n_results=top_k, include=[\"documents\", \"metadatas\", \"distances\"])\n",
        "    out = []\n",
        "    for doc, meta, dist in zip(res.get(\"documents\", [[]])[0], res.get(\"metadatas\", [[]])[0], res.get(\"distances\", [[]])[0]):\n",
        "        meta = dict(meta or {})\n",
        "        meta[\"score\"] = float(1 - dist)\n",
        "        out.append(Chunk(text=doc, metadata=meta))\n",
        "    return out\n",
        "\n",
        "def simple_conflict_scan(chunks: List[Chunk]) -> List[Tuple[Chunk, Chunk, str]]:\n",
        "    conflicts = []\n",
        "    def polarity(t: str) -> int:\n",
        "        t_low = t.lower()\n",
        "        pos = sum(1 for w in [\"shall\", \"must\", \"required\", \"entitled\"] if w in t_low)\n",
        "        neg = sum(1 for w in [\"shall not\", \"must not\", \"prohibited\", \"forbidden\", \"not permitted\", \"no liability\"] if w in t_low)\n",
        "        return pos - 2*neg\n",
        "    for i in range(len(chunks)):\n",
        "        for j in range(i+1, len(chunks)):\n",
        "            a, b = chunks[i], chunks[j]\n",
        "            ta = (a.metadata.get(\"section_title\") or \"\").lower()\n",
        "            tb = (b.metadata.get(\"section_title\") or \"\").lower()\n",
        "            if not ta or not tb:\n",
        "                continue\n",
        "            shared = set(re.findall(r\"[a-z]{4,}\", ta)).intersection(set(re.findall(r\"[a-z]{4,}\", tb)))\n",
        "            if not shared:\n",
        "                continue\n",
        "            pa, pb = polarity(a.text), polarity(b.text)\n",
        "            if (pa > 0 and pb < 0) or (pa < 0 and pb > 0):\n",
        "                conflicts.append((a, b, f\"Potential conflict on topic: {', '.join(sorted(shared))}\"))\n",
        "    return conflicts\n",
        "\n",
        "def make_citation(meta: Dict[str, Any]) -> str:\n",
        "    name = meta.get(\"doc_name\", \"Document\")\n",
        "    section = meta.get(\"section_title\")\n",
        "    pages = meta.get(\"pages\")\n",
        "    if section and pages:\n",
        "        return f\"{name} — {section} (pp. {pages})\"\n",
        "    if section:\n",
        "        return f\"{name} — {section}\"\n",
        "    if pages:\n",
        "        return f\"{name} (pp. {pages})\"\n",
        "    return name\n",
        "\n",
        "SYSTEM_PROMPT = \\\"\\\"\\\"You are a meticulous legal research assistant.\n",
        "You MUST answer using ONLY the provided context, citing each relevant section precisely.\n",
        "If the context is insufficient or conflicting, say so clearly and list the missing info or conflicts.\n",
        "Format citations inline like [Source 1], [Source 2], etc., and then provide a Source List mapping numbers to full citations.\n",
        "Keep the tone crisp and neutral. Avoid definitive legal advice; present findings with references.\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "def build_prompt(query: str, retrieved: List[Chunk]) -> Tuple[str, Dict[int, str]]:\n",
        "    sources_map = {}\n",
        "    context_blocks = []\n",
        "    for idx, ch in enumerate(retrieved, start=1):\n",
        "        sources_map[idx] = make_citation(ch.metadata)\n",
        "        context_blocks.append(f\"[Source {idx}] {ch.text}\")\n",
        "    context_text = \"\\\\n\\\\n\".join(context_blocks)\n",
        "    user_prompt = f\"User question:\\\\n{query}\\\\n\\\\nContext:\\\\n{context_text}\\\\n\\\\nInstructions: Provide a direct answer with inline citations like [Source 1], [Source 2]. After the answer, include a 'Source List' section mapping each source number to its citation.\"\n",
        "    return user_prompt, sources_map\n",
        "\n",
        "def call_openai(messages: List[Dict[str, str]], model: str, api_key: str, temperature: float = 0.0) -> str:\n",
        "    if OpenAI is None:\n",
        "        raise RuntimeError(f\"openai package not available: {OPENAI_IMPORT_ERROR}\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "def call_ollama(messages: List[Dict[str, str]], model: str = \"llama3.1\") -> str:\n",
        "    import json, urllib.request\n",
        "    req = urllib.request.Request(\"http://localhost:11434/api/chat\", method=\"POST\")\n",
        "    req.add_header(\"Content-Type\", \"application/json\")\n",
        "    payload = {\"model\": model, \"messages\": messages, \"stream\": False}\n",
        "    data = json.dumps(payload).encode(\"utf-8\")\n",
        "    try:\n",
        "        with urllib.request.urlopen(req, data, timeout=120) as r:\n",
        "            out = json.loads(r.read().decode(\"utf-8\"))\n",
        "            return out.get(\"message\", {}).get(\"content\", \"\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Ollama call failed: {e}\")\n",
        "\n",
        "# -------------- Streamlit UI --------------\n",
        "\n",
        "st.set_page_config(page_title=\"Multi-Document Legal RAG Assistant\", page_icon=\"⚖️\", layout=\"wide\")\n",
        "st.title(\"Multi-Document Legal Research Assistant (One-Shot RAG)\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "    files = st.file_uploader(\"Upload one or more files\", type=[\"pdf\", \"docx\"], accept_multiple_files=True)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    emb_model_name = st.selectbox(\n",
        "        \"Embedding model\",\n",
        "        [\"sentence-transformers/all-MiniLM-L6-v2\", \"sentence-transformers/all-MiniLM-L12-v2\", \"mixedbread-ai/mxbai-embed-large-v1\"],\n",
        "        index=0\n",
        "    )\n",
        "    chunk_chars = st.number_input(\"Max chunk size (chars)\", min_value=400, max_value=3000, value=1200, step=100)\n",
        "    overlap = st.number_input(\"Chunk overlap (chars)\", min_value=0, max_value=500, value=150, step=10)\n",
        "    top_k = st.number_input(\"Top-k retrieval\", min_value=2, max_value=12, value=6, step=1)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    gen_backend = st.selectbox(\"Backend\", [\"OpenAI\", \"Ollama (local)\"], index=0)\n",
        "    temperature = st.slider(\"Temperature\", 0.0, 1.0, 0.0, 0.05)\n",
        "    openai_model = st.selectbox(\"OpenAI model\", [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4.1-mini\", \"gpt-4.1\"], index=0, disabled=(gen_backend != \"OpenAI\"))\n",
        "    openai_key_input = st.text_input(\"OpenAI API Key (if using OpenAI)\", type=\"password\", placeholder=\"sk-...\")\n",
        "    ollama_model = st.text_input(\"Ollama model name (if using local)\", value=\"llama3.1\", disabled=(gen_backend != \"Ollama (local)\"))\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    persist_dir = st.text_input(\"Vector store path\", value=\"./rag_db\")\n",
        "    build_btn = st.button(\"Build / Refresh Index\", type=\"primary\")\n",
        "\n",
        "if \"built\" not in st.session_state:\n",
        "    st.session_state.built = False\n",
        "if \"collection_name\" not in st.session_state:\n",
        "    st.session_state.collection_name = f\"legal-rag-{uuid.uuid4().hex[:8]}\"\n",
        "if \"emb_model\" not in st.session_state:\n",
        "    st.session_state.emb_model = None\n",
        "if \"client\" not in st.session_state:\n",
        "    st.session_state.client = None\n",
        "\n",
        "if build_btn:\n",
        "    if not files:\n",
        "        st.error(\"Please upload at least one PDF/DOCX.\")\n",
        "    else:\n",
        "        with st.spinner(\"Parsing & indexing documents...\"):\n",
        "            emb_model = build_embeddings_model(emb_model_name)\n",
        "            st.session_state.emb_model = emb_model\n",
        "            client = ensure_vector_store(persist_dir)\n",
        "            st.session_state.client = client\n",
        "\n",
        "            all_chunks: List[Chunk] = []\n",
        "            for f in files:\n",
        "                name = f.name\n",
        "                data = f.read()\n",
        "                ext = name.lower().split(\".\")[-1]\n",
        "                if ext == \"pdf\":\n",
        "                    doc_text, page_map = extract_text_from_pdf(data)\n",
        "                elif ext == \"docx\":\n",
        "                    doc_text = extract_text_from_docx(data)\n",
        "                    page_map = {1: doc_text}\n",
        "                else:\n",
        "                    continue\n",
        "                doc_text = _clean_text(doc_text)\n",
        "                sections = detect_sections(doc_text)\n",
        "                for title, (s, e) in sections:\n",
        "                    sec_text = doc_text[s:e].strip()\n",
        "                    if not sec_text:\n",
        "                        continue\n",
        "                    smalls = split_into_chunks(sec_text, max_chars=chunk_chars, overlap=overlap)\n",
        "                    pages = None\n",
        "                    if ext == \"pdf\" and page_map:\n",
        "                        pages_hit = []\n",
        "                        for pg, t in page_map.items():\n",
        "                            if any(snippet.strip() and snippet.strip()[:60] in t for snippet in smalls[:2]):\n",
        "                                pages_hit.append(pg)\n",
        "                        if pages_hit:\n",
        "                            pages = f\"{min(pages_hit)}-{max(pages_hit)}\" if len(pages_hit) > 1 else f\"{pages_hit[0]}\"\n",
        "                    for small in smalls:\n",
        "                        all_chunks.append(Chunk(\n",
        "                            text=small,\n",
        "                            metadata={\n",
        "                                \"id\": str(uuid.uuid4()),\n",
        "                                \"doc_name\": name,\n",
        "                                \"section_title\": title[:120],\n",
        "                                \"pages\": pages,\n",
        "                            }\n",
        "                        ))\n",
        "            if not all_chunks:\n",
        "                st.error(\"No text extracted from uploaded files.\")\n",
        "            else:\n",
        "                add_to_collection(client, st.session_state.collection_name, emb_model, all_chunks)\n",
        "                st.session_state.built = True\n",
        "                st.success(f\"Indexed {len(all_chunks)} chunks from {len(files)} file(s).\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"Ask a legal question\")\n",
        "query = st.text_input(\"Enter your query\", placeholder=\"e.g., What are the termination rights and notice period?\")\n",
        "\n",
        "colA, colB = st.columns([1, 1])\n",
        "with colA:\n",
        "    run_btn = st.button(\"Retrieve & Answer\", type=\"primary\", use_container_width=True)\n",
        "with colB:\n",
        "    clear_btn = st.button(\"Reset Session\", use_container_width=True)\n",
        "if clear_btn:\n",
        "    st.session_state.built = False\n",
        "    st.session_state.emb_model = None\n",
        "    st.session_state.client = None\n",
        "    st.session_state.collection_name = f\"legal-rag-{uuid.uuid4().hex[:8]}\"\n",
        "    st.experimental_rerun()\n",
        "\n",
        "if run_btn:\n",
        "    if not st.session_state.built:\n",
        "        st.error(\"Please build the index first.\")\n",
        "    elif not query.strip():\n",
        "        st.error(\"Please enter a query.\")\n",
        "    else:\n",
        "        with st.spinner(\"Retrieving relevant sections...\"):\n",
        "            retrieved = retrieve(\n",
        "                st.session_state.client, st.session_state.collection_name,\n",
        "                st.session_state.emb_model, query, top_k=int(top_k)\n",
        "            )\n",
        "        if not retrieved:\n",
        "            st.warning(\"No relevant context found.\")\n",
        "        else:\n",
        "            conflicts = simple_conflict_scan(retrieved)\n",
        "            if conflicts:\n",
        "                with st.expander(\"Potential conflicts detected (heuristic)\"):\n",
        "                    for a, b, msg in conflicts:\n",
        "                        st.markdown(f\"- **{msg}** between *{make_citation(a.metadata)}* and *{make_citation(b.metadata)}*\")\n",
        "\n",
        "            user_prompt, sources_map = build_prompt(query, retrieved)\n",
        "            st.markdown(\"##### Retrieved Sources\")\n",
        "            for i, ch in enumerate(retrieved, start=1):\n",
        "                with st.expander(f\"[Source {i}] {make_citation(ch.metadata)} — score {ch.metadata.get('score', 0):.3f}\"):\n",
        "                    st.write(ch.text)\n",
        "\n",
        "            st.markdown(\"---\")\n",
        "            st.subheader(\"Answer\")\n",
        "\n",
        "            try:\n",
        "                if gen_backend == \"OpenAI\":\n",
        "                    api_key = openai_key_input or os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "                    if not api_key:\n",
        "                        st.error(\"OpenAI API key is required for the OpenAI backend.\")\n",
        "                        st.stop()\n",
        "                    answer = call_openai(\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                            {\"role\": \"user\", \"content\": user_prompt},\n",
        "                        ],\n",
        "                        model=openai_model,\n",
        "                        api_key=api_key,\n",
        "                        temperature=temperature,\n",
        "                    )\n",
        "                else:\n",
        "                    answer = call_ollama(\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                            {\"role\": \"user\", \"content\": user_prompt},\n",
        "                        ],\n",
        "                        model=ollama_model or \"llama3.1\",\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                st.error(f\"Generation failed: {e}\")\n",
        "                st.stop()\n",
        "\n",
        "            st.markdown(answer)\n",
        "            st.markdown(\"###### Source List\")\n",
        "            for i, cite in sources_map.items():\n",
        "                st.markdown(f\"- [Source {i}] {cite}\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "with st.expander(\"How to run (one-shot)\"):\n",
        "    st.markdown(\"Upload docs → Build Index → Ask → Answer\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)"
      ],
      "metadata": {
        "id": "fgG5DtUwYPPU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download cloudflared from Cloudflare's GitHub releases\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "\n",
        "# Install the .deb package\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "# Run Streamlit in background\n",
        "import subprocess, threading, time\n",
        "\n",
        "def run_streamlit():\n",
        "    subprocess.call([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"])\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "# Expose with Cloudflare Tunnel (gives free https://xxxx.trycloudflare.com)\n",
        "!cloudflared tunnel --url http://localhost:8501 --no-autoupdate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X60mTtL5YPMe",
        "outputId": "f849e6da-58b0-4c6c-834a-1e2af2502fa0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package cloudflared.\n",
            "(Reading database ... 126380 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2025.8.0) ...\n",
            "Setting up cloudflared (2025.8.0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[90m2025-08-19T08:30:50Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-08-19T08:30:50Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m |  https://ranger-ideal-core-stuffed.trycloudflare.com                                       |\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.8.0 (Checksum c7d3a69da0f7b9b1bc1ddcb0597d3552bcd7c15f8bbaba463dc489b94b7544ee)\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: e98600fe-b724-4445-bb9b-d13e1032da74\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-08-19T08:30:53Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.73\n",
            "2025/08/19 08:30:53 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-08-19T08:30:54Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m2e29145e-bb40-45d5-a7f7-d0fe469b41ec \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.73 \u001b[36mlocation=\u001b[0mord12 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-08-19T08:47:37Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2025-08-19T08:47:38Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.73\n",
            "\u001b[90m2025-08-19T08:47:38Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.73\n",
            "\u001b[90m2025-08-19T08:47:38Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2025-08-19T08:47:38Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2025-08-19T08:47:38Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zf0PKn6lbG-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}